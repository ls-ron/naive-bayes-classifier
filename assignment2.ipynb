{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dace5b20",
   "metadata": {},
   "source": [
    "### Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43a59f3",
   "metadata": {},
   "source": [
    "1. Data Representation & Preprocessing\n",
    "We start by representing each research description as a clean, token-based document. The text from the Description column is first lowercased then stripped of all punctuation and digits using a simple regular-expression filter. We use ntlk’s word_tokenize to split on whitespace and punctuation, producing a sequence of tokens. This token sequence is our basis for two feature representations:\n",
    "*Enhanced TF-IDF with Bigrams*: to capture local phrase structures, we extend our vocabulary to include both unigrams and bigrams. We apply document-frequency thresholds to filter out extremely rare or overly comon n‑grams, then weight each feature by TF‑IDF. This downweights generic tokens (e.g., \"the\", \"data\") and amplifies more discriminatve terms.\n",
    "2. Model Implementation and Improvements\n",
    "*Standard Naive Bayes*:\n",
    "We implement a multinomial Naive Bayes classifier from scratch:\n",
    "1. *Prior*: estimate class prior probabilities in log-space.\n",
    "2. *Likelihood*: for each class, sum term counts across all documents in that class, apply Laplace smoothing, and normalize\n",
    "3. *Prediction*: for a new document, sum the log-prior with the dot‑product of its feature vector and each class’s log-likelihood vector, choosing the class with the highest total score.\\\n",
    "*Improvements:*\n",
    "- including bigrams enables the model to recognise phrases that unigrams alone cannot.\n",
    "- removing tokens that appear in fewer than 5 documents or in more than 80% of all documents, we reduce noise/dimensionality.\n",
    "- term-frequency scaling coupled with inverse-document-frequency helps emphasise informative tokens and suppress common ones.\n",
    "These enhancements produce a richer feature set that often yields higher discriminative power while still maintaining computational efficiency.\n",
    "3. Evaluation Procedure\n",
    "We split train.csv into an 80% training/20% validation set. For improved model, we report:\n",
    "- overall fraction of correctly classified descriptions.\n",
    "Hyperparameters are via 3‑fold cross‑validation on the training split, optimizing validation accuracy.\n",
    "*Conclusion:* Our accuracy on the data was 98.18%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "704edac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for Assignment 2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a59fc3d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rony2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\rony2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6255de03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Data\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df  = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ab14359a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z\\s]\", \"\", text)\n",
    "    tokens = word_tokenize(text)\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3ab4d02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['clean'] = train_df['Description'].apply(clean_text)\n",
    "test_df ['clean'] = test_df ['Description'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5520139b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Feature Extraction (Baseline)\n",
    "vectorizer = CountVectorizer(binary=True)\n",
    "X_counts = vectorizer.fit_transform(train_df['clean'])\n",
    "y = train_df['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d01577ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes Classifier Implementation\n",
    "class NaiveBayesClassifier:\n",
    "    def __init__(self, alpha=1.0):\n",
    "        self.alpha = alpha\n",
    "    def fit(self, X, y):\n",
    "        n_docs, n_feats = X.shape\n",
    "        self.classes = np.unique(y)\n",
    "        self.log_prior = {}\n",
    "        self.log_likelihood = {}\n",
    "        for c in self.classes:\n",
    "            idx = np.where(y == c)[0]\n",
    "            X_c = X[idx]\n",
    "            self.log_prior[c] = np.log(len(idx) / n_docs)\n",
    "            counts = np.array(X_c.sum(axis=0)).flatten() + self.alpha\n",
    "            total = counts.sum()\n",
    "            self.log_likelihood[c] = np.log(counts / total)\n",
    "        return self    \n",
    "    def predict(self, X):\n",
    "        results = []\n",
    "        for i in range(X.shape[0]):\n",
    "            row = X[i].toarray().flatten()\n",
    "            scores = {c: self.log_prior[c] + (row * self.log_likelihood[c]).sum()\n",
    "                      for c in self.classes}\n",
    "            results.append(max(scores, key=scores.get))\n",
    "        return np.array(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4a3ca58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_counts, y, test_size=0.2, random_state=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "dcb90826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model with TF-IDF & Bigrams\n",
    "# CountVectorizer with bigrams + frequency filters\n",
    "erange = (1,2)\n",
    "vectorizer2 = CountVectorizer(ngram_range=erange, max_df=0.8, min_df=5)\n",
    "X_counts2 = vectorizer2.fit_transform(train_df['clean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d4d22229",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfTransformer()\n",
    "X_tfidf = tfidf.fit_transform(X_counts2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9f978b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap custom NB in sklearn interface\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "class SklearnNB(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, alpha=1.0):\n",
    "        self.alpha = alpha\n",
    "        self.model = NaiveBayesClassifier(alpha)\n",
    "    def fit(self, X, y):\n",
    "        self.model = self.model.fit(X, y)\n",
    "        return self\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "18b663e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best smoothing alpha: 0.1\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning for alpha\n",
    "param_grid = {'alpha': [0.1, 0.5, 1.0, 2.0]}\n",
    "grid = GridSearchCV(SklearnNB(), param_grid, cv=3, scoring='accuracy')\n",
    "grid.fit(X_tfidf, y)\n",
    "best_alpha = grid.best_params_['alpha']\n",
    "print(\"Best smoothing alpha:\", best_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "96315b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved Accuracy: 0.9840909090909091\n"
     ]
    }
   ],
   "source": [
    "# Evaluate improved on validation split\n",
    "X_train2, X_val2, y_train2, y_val2 = train_test_split(X_tfidf, y, test_size=0.2, random_state=777)\n",
    "imp_clf = NaiveBayesClassifier(alpha=best_alpha)\n",
    "imp_clf.fit(X_train2, y_train2)\n",
    "imp_preds = imp_clf.predict(X_val2)\n",
    "print(\"Improved Accuracy:\", accuracy_score(y_val2, imp_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "396d7667",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.NaiveBayesClassifier at 0x16fce9ec250>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final Training\n",
    "full_counts = vectorizer2.fit_transform(train_df['clean'])\n",
    "full_tfidf   = tfidf.fit_transform(full_counts)\n",
    "final_clf = NaiveBayesClassifier(alpha=best_alpha)\n",
    "final_clf.fit(full_tfidf, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f12ed0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test set\n",
    "X_test_counts = vectorizer2.transform(test_df['clean'])\n",
    "X_test_tfidf   = tfidf.transform(X_test_counts)\n",
    "test_preds = final_clf.predict(X_test_tfidf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
